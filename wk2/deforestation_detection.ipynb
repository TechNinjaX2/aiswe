# %% [markdown]
# Deforestation Detection for SDG 15 — Jupyter-style Python Notebook

# Objective
# Use unsupervised + supervised deep learning to detect deforestation in satellite imagery. This notebook contains code, a 1-page report and a 5-minute presentation script.

# Notes before running
# 1. Dataset: download a satellite imagery dataset from Kaggle containing 'deforested' and 'forest' images or time-series images. Examples: "Planet dataset" or any landcover / deforestation dataset. Place images into two folders: data/forest/ and data/deforested/ or set `DATA_DIR` appropriately.
# 2. This notebook uses CPU/GPU if available. For large datasets, run on a GPU runtime.
# 3. This pipeline demonstrates:
#    - Preprocessing (resize, normalize, augment)
#    - Unsupervised feature learning with a convolutional autoencoder
#    - Clustering encoded features (KMeans) to find deforestation clusters
#    - Supervised baseline models: Decision Tree on extracted features and a CNN classifier
#    - Evaluation: accuracy, MAE (on probabilities), F1-score, confusion matrix
#    - Visualizations and a short report + presentation text

# %%
# Imports
import os
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error, classification_report
from sklearn.tree import DecisionTreeClassifier
import tensorflow as tf
from tensorflow.keras import layers, models, losses, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

# Set seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# %% [markdown]
# Configuration — update paths and parameters as needed
# %%
DATA_DIR = 'data'  # expects subfolders: data/forest/, data/deforested/
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS_AE = 20  # autoencoder
EPOCHS_CLS = 15  # classifier
N_CLUSTERS = 2  # deforested vs forest

# %% [markdown]
# Utility functions to load images into arrays
# %%
from tensorflow.keras.preprocessing import image

def load_image_paths(data_dir):
    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
    paths = []
    labels = []
    for idx, cls in enumerate(classes):
        cls_path = os.path.join(data_dir, cls)
        for fname in os.listdir(cls_path):
            if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):
                paths.append(os.path.join(cls_path, fname))
                labels.append(idx)
    return paths, np.array(labels), classes


def load_and_preprocess(img_path, target_size=IMG_SIZE):
    img = image.load_img(img_path, target_size=target_size)
    arr = image.img_to_array(img)
    arr = arr / 255.0  # normalize to [0,1]
    return arr

# %% [markdown]
# Load dataset paths & sample count
# %%
paths, labels, classes = load_image_paths(DATA_DIR)
print(f"Found classes: {classes}")
print(f"Total images: {len(paths)}")

if len(paths) == 0:
    print("No images found. Please put images under data/forest/ and data/deforested/ and re-run.")

# %% [markdown]
# Build arrays (careful with memory for large datasets). If dataset large, use generators instead.
# %%
MAX_LOAD = None  # set to an integer to limit memory usage for testing, or None to load all
if MAX_LOAD:
    paths = paths[:MAX_LOAD]
    labels = labels[:MAX_LOAD]

X = np.array([load_and_preprocess(p) for p in paths])
y = labels

print('X shape:', X.shape, 'y shape:', y.shape)

# %% [markdown]
# Split data
# %%
X_train, X_test, y_train, y_test, paths_train, paths_test = train_test_split(X, y, paths, test_size=0.2, random_state=SEED, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=SEED, stratify=y_train)

print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)

# %% [markdown]
# Data augmentation generator (for supervised training)
# %%
train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=0.1)
val_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)
val_generator = val_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)

# %% [markdown]
# 1) Unsupervised: Convolutional Autoencoder to learn compact representations
# %%
input_shape = IMG_SIZE + (3,)

def build_conv_autoencoder(input_shape, latent_dim=128):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
    x = layers.MaxPooling2D(2, padding='same')(x)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2, padding='same')(x)
    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2, padding='same')(x)
    shape_before_flatten = tf.keras.backend.int_shape(x)[1:]
    x = layers.Flatten()(x)
    latent = layers.Dense(latent_dim, name='latent')(x)

    # Decoder
    x = layers.Dense(np.prod(shape_before_flatten))(latent)
    x = layers.Reshape(shape_before_flatten)(x)
    x = layers.Conv2DTranspose(128, 3, strides=1, activation='relu', padding='same')(x)
    x = layers.UpSampling2D(2)(x)
    x = layers.Conv2DTranspose(64, 3, strides=1, activation='relu', padding='same')(x)
    x = layers.UpSampling2D(2)(x)
    x = layers.Conv2DTranspose(32, 3, strides=1, activation='relu', padding='same')(x)
    x = layers.UpSampling2D(2)(x)
    outputs = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)

    autoencoder = models.Model(inputs, outputs, name='conv_autoencoder')
    encoder = models.Model(inputs, latent, name='encoder')
    return autoencoder, encoder

autoencoder, encoder = build_conv_autoencoder(input_shape, latent_dim=128)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()

# %% [markdown]
# Train autoencoder (unsupervised) — fit on training images (no labels required)
# %%
history_ae = autoencoder.fit(X_train, X_train, epochs=EPOCHS_AE, batch_size=BATCH_SIZE, validation_data=(X_val, X_val))

# %% [markdown]
# Visualize reconstruction examples
# %%
n = min(5, X_test.shape[0])
recon = autoencoder.predict(X_test[:n])
plt.figure(figsize=(12, 4))
for i in range(n):
    plt.subplot(2, n, i+1)
    plt.imshow(X_test[i])
    plt.axis('off')
    plt.subplot(2, n, n+i+1)
    plt.imshow(recon[i])
    plt.axis('off')
plt.suptitle('Top: original, Bottom: reconstruction')
plt.show()

# %% [markdown]
# 2) Clustering encoded features (K-means) — unsupervised detection of deforestation patterns
# %%
features_train = encoder.predict(X_train)
features_test = encoder.predict(X_test)

kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=SEED)
cluster_labels_train = kmeans.fit_predict(features_train)
cluster_labels_test = kmeans.predict(features_test)

# Evaluate how well clusters match true labels (if available)
# Map clusters to labels by majority vote
from collections import Counter
cluster_to_label = {}
for c in range(N_CLUSTERS):
    # find majority true label in this cluster
    idxs = np.where(cluster_labels_train == c)[0]
    if len(idxs) == 0:
        cluster_to_label[c] = 0
    else:
        most_common = Counter(y_train[idxs]).most_common(1)[0][0]
        cluster_to_label[c] = most_common

pred_from_cluster = np.array([cluster_to_label[c] for c in cluster_labels_test])
print('Clustering accuracy (mapped):', accuracy_score(y_test, pred_from_cluster))
print('Clustering F1 (mapped):', f1_score(y_test, pred_from_cluster, average='weighted'))

# %% [markdown]
# 3) Supervised baseline: Decision Tree using encoded features
# %%
clf_dt = DecisionTreeClassifier(random_state=SEED)
clf_dt.fit(features_train, y_train)

pred_dt = clf_dt.predict(features_test)
print('Decision Tree accuracy:', accuracy_score(y_test, pred_dt))
print('Decision Tree F1:', f1_score(y_test, pred_dt, average='weighted'))
print(classification_report(y_test, pred_dt, target_names=classes))

# Mean Absolute Error on decision tree (we'll treat labels numerically for MAE)
mae_dt = mean_absolute_error(y_test, pred_dt)
print('Decision Tree MAE:', mae_dt)

# %% [markdown]
# 4) Supervised CNN classifier (end-to-end)
# %%

def build_simple_cnn(input_shape, n_classes):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(128, activation='relu')(x)
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    model = models.Model(inputs, outputs, name='simple_cnn')
    return model

n_classes = len(classes)
cnn = build_simple_cnn(input_shape, n_classes)
cnn.compile(optimizer=optimizers.Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
cnn.summary()

history_cnn = cnn.fit(train_generator, epochs=EPOCHS_CLS, validation_data=val_generator)

# %% [markdown]
# Evaluate CNN on test set
# %%
pred_probs = cnn.predict(X_test)
preds = np.argmax(pred_probs, axis=1)
print('CNN accuracy:', accuracy_score(y_test, preds))
print('CNN F1:', f1_score(y_test, preds, average='weighted'))
print(classification_report(y_test, preds, target_names=classes))
print('CNN MAE (probabilities vs labels):', mean_absolute_error(y_test, preds))

# Confusion matrix
cm = confusion_matrix(y_test, preds)
plt.figure(figsize=(6,5))
plt.imshow(cm, interpolation='nearest')
plt.title('Confusion matrix')
plt.colorbar()
plt.xticks(range(n_classes), classes, rotation=45)
plt.yticks(range(n_classes), classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# %% [markdown]
# 5) Combine unsupervised + supervised: use autoencoder features with a small classifier
# %%
from sklearn.linear_model import LogisticRegression
features_train = encoder.predict(X_train)
features_val = encoder.predict(X_val)
features_test = encoder.predict(X_test)

logreg = LogisticRegression(max_iter=500, random_state=SEED)
logreg.fit(features_train, y_train)
logreg_pred = logreg.predict(features_test)
print('Logistic regression on encoded features acc:', accuracy_score(y_test, logreg_pred))
print('F1:', f1_score(y_test, logreg_pred, average='weighted'))

# %% [markdown]
# Save models (optional)
# %%
MODEL_DIR = 'models'
os.makedirs(MODEL_DIR, exist_ok=True)
autoencoder.save(os.path.join(MODEL_DIR, 'autoencoder.h5'))
encoder.save(os.path.join(MODEL_DIR, 'encoder.h5'))
cnn.save(os.path.join(MODEL_DIR, 'cnn_classifier.h5'))

# Export cluster labels for visualization or GIS overlay (example CSV)
import pandas as pd
out_df = pd.DataFrame({'path': paths_test, 'true_label': y_test, 'cnn_pred': preds, 'kmeans_cluster': cluster_labels_test})
out_df.to_csv('prediction_results.csv', index=False)
print('Saved prediction_results.csv')

# %% [markdown]
# 6) Simple visualizations of clustered patches and examples of false positives/negatives
# %%
# Show random examples of each predicted class
for cls_idx, cls_name in enumerate(classes):
    idxs = np.where(preds == cls_idx)[0]
    if len(idxs) == 0:
        continue
    sample_idxs = np.random.choice(idxs, min(4, len(idxs)), replace=False)
    plt.figure(figsize=(8,2))
    for i, si in enumerate(sample_idxs):
        plt.subplot(1, len(sample_idxs), i+1)
        plt.imshow(X_test[si])
        plt.title(f'True {classes[y_test[si]]}')
        plt.axis('off')
    plt.suptitle(f'CNN predicted: {cls_name}')
    plt.show()

# %% [markdown]
# 1-page Report (Markdown) — included here so this single file contains everything requested
# %% [markdown]
# SDG 15 Problem Addressed

This project addresses **SDG 15 — Life on Land**, focusing on detecting and monitoring deforestation using satellite imagery. Rapid and undetected deforestation results in biodiversity loss, reduced carbon sequestration, and ecosystem degradation. Timely automated detection helps policymakers and conservationists prioritize enforcement and reforestation efforts.

# ML Approach Used

**Unsupervised feature learning + clustering + supervised classification**

- **Convolutional Autoencoder (unsupervised)** to learn compact, high-level features from satellite image patches without labels.
- **K-Means clustering** on encoded features to identify natural groupings; clusters are analyzed and mapped to deforested vs forested regions using majority-vote mapping.
- **Decision Tree** trained on encoded features serves as an interpretable baseline.
- **CNN classifier** (supervised end-to-end) trained on labeled examples provides a higher-accuracy detector when labels are available.
- **Logistic Regression** on encoded features demonstrates how unsupervised representation improves simple classifiers.

# Results (example—your numbers will vary based on dataset)

- **Unsupervised clustering (KMeans)** mapped to labels: *clustering accuracy (mapped)* ≈ *reported by run*.
- **Decision Tree baseline**: accuracy ≈ *reported by run*, MAE ≈ *reported by run*, F1 ≈ *reported by run*.
- **CNN classifier**: accuracy ≈ *reported by run*, F1-score ≈ *reported by run*.

(See printed metrics in the notebook; results depend heavily on dataset quality, class balance, image resolution, and pre/post processing.)

# Ethical Considerations

- **Data bias & representativeness**: Satellite datasets may over- or under-represent specific regions, seasons, or sensor types — leading to biased detection performance across geographies.
- **False positives / negatives**: Incorrect flags may lead to misguided enforcement actions or missed deforestation events. Deployments must include human-in-the-loop verification.
- **Privacy**: Although satellite data are typically coarse, ensure compliance with local laws when higher-resolution imagery is used near settlements.
- **Environmental & social context**: Not all tree loss is harmful (e.g., planned agricultural cycles). Model outputs must be combined with land-use metadata.
- **Sustainability**: Model training should consider compute footprint. Prefer lightweight models or on-device inference for repeated runs.

# 5-minute Presentation / Demo Script + Slide Outline

This section contains a short script and slide contents (5 slides) suitable for a 5-minute demo.

**Slide 1 — Title & Problem (30s)**
- Title: Deforestation Detection for SDG 15
- One-liner: Automating detection of deforestation from satellite imagery to help conservation.

**Slide 2 — Data & Preprocessing (45s)**
- Show sample images (forest vs deforested), explain normalization, resizing to 128x128, and augmentation.

**Slide 3 — ML Approach (60s)**
- Visual diagram: Autoencoder -> Encoded features -> KMeans clustering; parallel: CNN classifier and Decision Tree baseline.
- Explain why unsupervised representation helps when labels are scarce.

**Slide 4 — Results & Visualizations (60s)**
- Present key metrics from notebook (accuracy, F1) and confusion matrix.
- Show sample true/false positives and clustering output.

**Slide 5 — Ethical Considerations & Next Steps (45s)**
- Summarize ethical points (bias, human-in-loop, validation) and next steps: integrate time-series analysis, higher resolution, deploy as monitoring dashboard (e.g., Streamlit + GIS overlays).

**Demo Notes**
- Live demo: run a small folder of test images through the `cnn.predict()` and show a simple map overlay (if georeferenced) or grid visualization.
- End with how this supports SDG 15: faster detection -> targeted action -> less biodiversity loss.

# %% [markdown]
# Final Notes & How to Reproduce

1. Get dataset from Kaggle (search for deforestation, landcover, or Planet dataset). Unzip into `data/` with subfolders for classes.
2. Install requirements: `pip install tensorflow scikit-learn matplotlib pandas pillow` (preferably in a venv).
3. Run the cells top-to-bottom. For large datasets, use generators and consider using a GPU runtime.
4. To deploy: export `cnn_classifier.h5` and wrap in a Streamlit app that accepts images and returns predictions; overlay geolocation if available.

# End of notebook

